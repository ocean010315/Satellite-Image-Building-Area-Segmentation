{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Segmentation Task에서 가장 기초적이고 대표적인 'Unet' 구조를 활용하여 구현\n",
        "\n",
        "RLE 인코딩과 RLE 디코딩에 대한 코드 포함\n",
        "\n",
        "DeepLab v3+로 변경"
      ],
      "metadata": {
        "id": "RgtS6rN5fOVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ2ViFv6M65B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from io import BytesIO\n",
        "import tarfile\n",
        "import tempfile\n",
        "from six.moves import urllib\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Union\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Utils**"
      ],
      "metadata": {
        "id": "kPBhGa0zflBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RLE 디코딩 함수\n",
        "def rle_decode(mask_rle: Union[str, int], shape=(224, 224)) -> np.array:\n",
        "    '''\n",
        "    mask_rle: run-length as string formatted (start length)\n",
        "    shape: (height,width) of array to return\n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "    '''\n",
        "    if mask_rle == -1:\n",
        "        return np.zeros(shape)\n",
        "\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape)\n",
        "\n",
        "def dice_score(prediction: np.array, ground_truth: np.array, smooth=1e-7) -> float:\n",
        "    '''\n",
        "    Calculate Dice Score between two binary masks.\n",
        "    '''\n",
        "    intersection = np.sum(prediction * ground_truth)\n",
        "    return (2.0 * intersection + smooth) / (np.sum(prediction) + np.sum(ground_truth) + smooth)\n",
        "\n",
        "\n",
        "def calculate_dice_scores(ground_truth_df, prediction_df, img_shape=(224, 224)) -> List[float]:\n",
        "    '''\n",
        "    Calculate Dice scores for a dataset.\n",
        "    '''\n",
        "\n",
        "\n",
        "    # Keep only the rows in the prediction dataframe that have matching img_ids in the ground truth dataframe\n",
        "    prediction_df = prediction_df[prediction_df.iloc[:, 0].isin(ground_truth_df.iloc[:, 0])]\n",
        "    prediction_df.index = range(prediction_df.shape[0])\n",
        "\n",
        "\n",
        "    # Extract the mask_rle columns\n",
        "    pred_mask_rle = prediction_df.iloc[:, 1]\n",
        "    gt_mask_rle = ground_truth_df.iloc[:, 1]\n",
        "\n",
        "\n",
        "    def calculate_dice(pred_rle, gt_rle):\n",
        "        pred_mask = rle_decode(pred_rle, img_shape)\n",
        "        gt_mask = rle_decode(gt_rle, img_shape)\n",
        "\n",
        "\n",
        "        if np.sum(gt_mask) > 0 or np.sum(pred_mask) > 0:\n",
        "            return dice_score(pred_mask, gt_mask)\n",
        "        else:\n",
        "            return None  # No valid masks found, return None\n",
        "\n",
        "\n",
        "    dice_scores = Parallel(n_jobs=-1)(\n",
        "        delayed(calculate_dice)(pred_rle, gt_rle) for pred_rle, gt_rle in zip(pred_mask_rle, gt_mask_rle)\n",
        "    )\n",
        "\n",
        "\n",
        "    dice_scores = [score for score in dice_scores if score is not None]  # Exclude None values\n",
        "\n",
        "\n",
        "    return np.mean(dice_scores)\n",
        "\n",
        "# RLE 인코딩 함수\n",
        "def rle_encode(mask):\n",
        "    pixels = mask.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)"
      ],
      "metadata": {
        "id": "CnkYIJv7bybE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Dataset**"
      ],
      "metadata": {
        "id": "eIUZ3-mRfncw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SatelliteDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, infer=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "        self.infer = infer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data.iloc[idx, 1]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.infer:\n",
        "            if self.transform:\n",
        "                image = self.transform(image=image)['image']\n",
        "            return image\n",
        "\n",
        "        mask_rle = self.data.iloc[idx, 2]\n",
        "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "OKvtNu4ab1_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loader**"
      ],
      "metadata": {
        "id": "kXGy8G4HftfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Info.\n",
        "\n",
        "### train_img [폴더]\n",
        "TRAIN_0000.png ~ TRAIN_7139.png\n",
        "\n",
        "\n",
        "### test_img [폴더]\n",
        "TEST_00000.png ~ TEST_60639.png\n",
        "\n",
        "\n",
        "### train.csv [파일]\n",
        "img_id : 학습 위성 이미지 샘플 ID\n",
        "img_path : 학습 위성 이미지 경로 (상대 경로)\n",
        "mask_rle : RLE 인코딩된 이진마스크(0 : 배경, 1 : 건물) 정보\n",
        "학습 위성 이미지에는 반드시 건물이 포함되어 있습니다.\n",
        "그러나 추론 위성 이미지에는 건물이 포함되어 있지 않을 수 있습니다.\n",
        "학습 위성 이미지의 해상도는 0.5m/픽셀이며, 추론 위성 이미지의 해상도는 공개하지 않습니다.\n",
        "\n",
        "\n",
        "### test.csv [파일]\n",
        "img_id : 추론 위성 이미지 샘플 ID\n",
        "img_path : 추론 위성 이미지 경로 (상대 경로)\n",
        "\n",
        "\n",
        "### sample_submission.csv [파일] - 제출 양식\n",
        "img_id : 추론 위성 이미지 샘플 ID\n",
        "mask_rle : RLE 인코딩된 예측 이진마스크(0: 배경, 1 : 건물) 정보\n",
        "단, 예측 결과에 건물이 없는 경우 반드시 -1 처리"
      ],
      "metadata": {
        "id": "wwDEBm2qe5Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "path = \"/content/gdrive/My Drive/ai_dataset\"\n",
        "\n",
        "file_list = os.listdir(path)\n",
        "file_list_py = [file for file in file_list]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjwSVcqFhJic",
        "outputId": "d30394db-e219-45fb-899c-5a793de7fc03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset = SatelliteDataset(csv_file='/content/gdrive/MyDrive/ai_dataset/train.csv', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "id": "qyIwbYXzb3Ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eecfd1b5-01d7-419e-dfbd-9b73fb6f7f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Model**"
      ],
      "metadata": {
        "id": "xxy_0z_Vfxr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepLabV3+ 구조 정의\n",
        "class DeepLabV3Plus(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DeepLabV3Plus, self).__init__()\n",
        "        self.backbone = models.resnet50(pretrained=True) # backbone network Xception으로 수정\n",
        "        self.asspp = ASSPP() # Astrous Separable Spatial Pyramid convolution\n",
        "        self.decoder = Decoder() # Decoder\n",
        "        '''\n",
        "        구조의 마지막 layer인 로짓 생성 레이어의 정의부\n",
        "        ASSPP와 Decoder를 거친 feature map을 입력으로 받아 클래스 수에 해당하는 출력 채널 수를 가진 로짓 생성\n",
        "        '''\n",
        "        self.logits = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone.conv1(x)\n",
        "        x = self.backbone.bn1(x)\n",
        "        x = self.backbone.relu(x)\n",
        "        x = self.backbone.maxpool(x)\n",
        "\n",
        "        x = self.backbone.layer1(x)\n",
        "        x = self.backbone.layer2(x)\n",
        "        x = self.backbone.layer3(x)\n",
        "        x = self.backbone.layer4(x)\n",
        "\n",
        "        x = self.asspp(x)\n",
        "        x = self.decoder(x, x)\n",
        "        x = self.logits(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ASSPP(nn.Module): # Atrous Separable Spatial Pyramid Pooling\n",
        "    def __init__(self, in_channels=2048, out_channels=256):\n",
        "        super(ASSPP, self).__init__()\n",
        "        dilations = [1, 6, 12, 18] # rate\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1) # Atrous Conv\n",
        "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilations[0] + 1, dilation=dilations[0]) # 1x1 Conv\n",
        "        self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilations[1] + 1, dilation=dilations[1]) # 3X3 Conv rate 6\n",
        "        self.conv4 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilations[2] + 1, dilation=dilations[2]) # 3x3 Conv rate 12\n",
        "        self.conv5 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=dilations[3] + 1, dilation=dilations[3]) # 3x3 Conv rate 18\n",
        "\n",
        "        self.sep_conv1 = nn.Conv2d(out_channels * 5, out_channels, kernel_size=1)\n",
        "        self.sep_conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, groups=out_channels)\n",
        "        self.sep_conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        convolution을 통해 처리된 feature map 결과들을 channel 차원을 기준으로 결합(torch.cat 이용)\n",
        "        '''\n",
        "        feat1 = self.conv1(x)\n",
        "        feat2 = self.conv2(x)\n",
        "        feat3 = self.conv3(x)\n",
        "        feat4 = self.conv4(x)\n",
        "        feat5 = self.conv5(x)\n",
        "\n",
        "        # 특성 맵들의 크기 확인\n",
        "        print(feat1.shape, feat2.shape, feat3.shape, feat4.shape, feat5.shape)\n",
        "\n",
        "        # 특성 맵들의 크기를 동일하게 조정\n",
        "        feat1 = nn.AdaptiveAvgPool2d((feat5.size(2), feat5.size(3)))(feat1)\n",
        "        feat2 = nn.AdaptiveAvgPool2d((feat5.size(2), feat5.size(3)))(feat2)\n",
        "        feat3 = nn.AdaptiveAvgPool2d((feat5.size(2), feat5.size(3)))(feat3)\n",
        "        feat4 = nn.AdaptiveAvgPool2d((feat5.size(2), feat5.size(3)))(feat4)\n",
        "\n",
        "        out = torch.cat([feat1, feat2, feat3, feat4, feat5], dim=1)\n",
        "\n",
        "        # 결합된 특성 맵은 sep_conv1을 통해 채널 방향으로 축소\n",
        "        # sep_conv2는 3x3커널을 사용, 그룹 내에서 채널별로 분리하여 공간 방향의 특성을 잡아냄\n",
        "        # sep_conv3를 통해 특성을 다시 축소한 후 ReLU 활성화 함수 적용해 최종 출력 얻어냄\n",
        "        out = self.sep_conv1(out)\n",
        "        out = self.sep_conv2(out)\n",
        "        out = self.sep_conv3(out)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels=256, out_channels=48, low_level_channels=256):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels + low_level_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, low_level_features):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = F.interpolate(out, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        out = torch.cat([out, low_level_features], dim=1)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "0ZH4pWqn6xuq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Train**"
      ],
      "metadata": {
        "id": "OKt9zylqf0Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model 초기화\n",
        "model = DeepLabV3Plus(3).to(device)\n",
        "\n",
        "# loss function과 optimizer 정의\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(10):  # 10 에폭 동안 학습합니다.\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, masks in tqdm(dataloader):\n",
        "        images = images.float().to(device)\n",
        "        masks = masks.float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')"
      ],
      "metadata": {
        "id": "aqUhHq_bb9iR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "outputId": "9b162d4e-be48-4c5a-b50e-2be3cc4f44a0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "  0%|          | 0/447 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 256, 7, 7]) torch.Size([16, 256, 9, 9]) torch.Size([16, 256, 9, 9]) torch.Size([16, 256, 9, 9]) torch.Size([16, 256, 9, 9])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/447 [00:11<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e6c493f3080f>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-c547204f5fc7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masspp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-c547204f5fc7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, low_level_features)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_level_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 18 but got size 9 for tensor number 1 in the list."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference**"
      ],
      "metadata": {
        "id": "-pD2CtOSf49S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SatelliteDataset(csv_file='/content/gdrive/MyDrive/ai_dataset/test.csv', transform=transform, infer=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "hh9VB0EjcBQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    result = []\n",
        "    for images in tqdm(test_dataloader):\n",
        "        images = images.float().to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
        "        masks = np.squeeze(masks, axis=1)\n",
        "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            mask_rle = rle_encode(masks[i])\n",
        "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
        "                result.append(-1)\n",
        "            else:\n",
        "                result.append(mask_rle)"
      ],
      "metadata": {
        "id": "CtPNk1E9cEhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Submisssion**"
      ],
      "metadata": {
        "id": "k5up68J8f7YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('/content/gdrive/MyDrive/ai_dataset/sample_submission.csv')\n",
        "submit['mask_rle'] = result"
      ],
      "metadata": {
        "id": "RJNQLImmcFB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit.to_csv('/content/gdrive/MyDrive/ai_dataset/submit.csv', index=False)"
      ],
      "metadata": {
        "id": "XVvSekaKcHNN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}