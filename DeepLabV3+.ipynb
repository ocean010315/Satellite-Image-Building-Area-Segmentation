{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Segmentation Task에서 가장 기초적이고 대표적인 'Unet' 구조를 활용하여 구현\n",
        "\n",
        "RLE 인코딩과 RLE 디코딩에 대한 코드 포함\n",
        "\n",
        "DeepLab v3+로 변경"
      ],
      "metadata": {
        "id": "RgtS6rN5fOVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ2ViFv6M65B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from io import BytesIO\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import tensorflow as tf\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Union\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Utils**"
      ],
      "metadata": {
        "id": "kPBhGa0zflBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RLE 디코딩 함수\n",
        "def rle_decode(mask_rle: Union[str, int], shape=(224, 224)) -> np.array:\n",
        "    '''\n",
        "    mask_rle: run-length as string formatted (start length)\n",
        "    shape: (height,width) of array to return\n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "    '''\n",
        "    if mask_rle == -1:\n",
        "        return np.zeros(shape)\n",
        "\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape)\n",
        "\n",
        "def dice_score(prediction: np.array, ground_truth: np.array, smooth=1e-7) -> float:\n",
        "    '''\n",
        "    Calculate Dice Score between two binary masks.\n",
        "    '''\n",
        "    intersection = np.sum(prediction * ground_truth)\n",
        "    return (2.0 * intersection + smooth) / (np.sum(prediction) + np.sum(ground_truth) + smooth)\n",
        "\n",
        "\n",
        "def calculate_dice_scores(ground_truth_df, prediction_df, img_shape=(224, 224)) -> List[float]:\n",
        "    '''\n",
        "    Calculate Dice scores for a dataset.\n",
        "    '''\n",
        "\n",
        "\n",
        "    # Keep only the rows in the prediction dataframe that have matching img_ids in the ground truth dataframe\n",
        "    prediction_df = prediction_df[prediction_df.iloc[:, 0].isin(ground_truth_df.iloc[:, 0])]\n",
        "    prediction_df.index = range(prediction_df.shape[0])\n",
        "\n",
        "\n",
        "    # Extract the mask_rle columns\n",
        "    pred_mask_rle = prediction_df.iloc[:, 1]\n",
        "    gt_mask_rle = ground_truth_df.iloc[:, 1]\n",
        "\n",
        "\n",
        "    def calculate_dice(pred_rle, gt_rle):\n",
        "        pred_mask = rle_decode(pred_rle, img_shape)\n",
        "        gt_mask = rle_decode(gt_rle, img_shape)\n",
        "\n",
        "\n",
        "        if np.sum(gt_mask) > 0 or np.sum(pred_mask) > 0:\n",
        "            return dice_score(pred_mask, gt_mask)\n",
        "        else:\n",
        "            return None  # No valid masks found, return None\n",
        "\n",
        "\n",
        "    dice_scores = Parallel(n_jobs=-1)(\n",
        "        delayed(calculate_dice)(pred_rle, gt_rle) for pred_rle, gt_rle in zip(pred_mask_rle, gt_mask_rle)\n",
        "    )\n",
        "\n",
        "\n",
        "    dice_scores = [score for score in dice_scores if score is not None]  # Exclude None values\n",
        "\n",
        "\n",
        "    return np.mean(dice_scores)\n",
        "\n",
        "# RLE 인코딩 함수\n",
        "def rle_encode(mask):\n",
        "    pixels = mask.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)"
      ],
      "metadata": {
        "id": "CnkYIJv7bybE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Dataset**"
      ],
      "metadata": {
        "id": "eIUZ3-mRfncw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SatelliteDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None, infer=False):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "        self.infer = infer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data.iloc[idx, 1]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.infer:\n",
        "            if self.transform:\n",
        "                image = self.transform(image=image)['image']\n",
        "            return image\n",
        "\n",
        "        mask_rle = self.data.iloc[idx, 2]\n",
        "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "OKvtNu4ab1_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loader**"
      ],
      "metadata": {
        "id": "kXGy8G4HftfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Info.\n",
        "\n",
        "### train_img [폴더]\n",
        "TRAIN_0000.png ~ TRAIN_7139.png\n",
        "\n",
        "\n",
        "### test_img [폴더]\n",
        "TEST_00000.png ~ TEST_60639.png\n",
        "\n",
        "\n",
        "### train.csv [파일]\n",
        "img_id : 학습 위성 이미지 샘플 ID\n",
        "img_path : 학습 위성 이미지 경로 (상대 경로)\n",
        "mask_rle : RLE 인코딩된 이진마스크(0 : 배경, 1 : 건물) 정보\n",
        "학습 위성 이미지에는 반드시 건물이 포함되어 있습니다.\n",
        "그러나 추론 위성 이미지에는 건물이 포함되어 있지 않을 수 있습니다.\n",
        "학습 위성 이미지의 해상도는 0.5m/픽셀이며, 추론 위성 이미지의 해상도는 공개하지 않습니다.\n",
        "\n",
        "\n",
        "### test.csv [파일]\n",
        "img_id : 추론 위성 이미지 샘플 ID\n",
        "img_path : 추론 위성 이미지 경로 (상대 경로)\n",
        "\n",
        "\n",
        "### sample_submission.csv [파일] - 제출 양식\n",
        "img_id : 추론 위성 이미지 샘플 ID\n",
        "mask_rle : RLE 인코딩된 예측 이진마스크(0: 배경, 1 : 건물) 정보\n",
        "단, 예측 결과에 건물이 없는 경우 반드시 -1 처리"
      ],
      "metadata": {
        "id": "wwDEBm2qe5Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "path = \"/content/gdrive/My Drive/ai_dataset\"\n",
        "\n",
        "file_list = os.listdir(path)\n",
        "file_list_py = [file for file in file_list]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjwSVcqFhJic",
        "outputId": "c421a453-c671-472f-aeb1-0d55a2ced2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(224, 224),\n",
        "        A.RandomRotate90(),\n",
        "        A.Flip(),\n",
        "        A.OneOf([\n",
        "            A.IAAAdditiveGaussianNoise(),\n",
        "            A.GaussNoise(),\n",
        "        ], p=0.5),\n",
        "        A.HueSaturationValue(),\n",
        "        A.CLAHE(),\n",
        "        A.OpticalDistortion(),\n",
        "        A.RandomContrast(),\n",
        "        A.RandomBrightness(),\n",
        "        A.IAAEmboss(),\n",
        "        A.MotionBlur(),\n",
        "        A.Normalize(),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset = SatelliteDataset(csv_file='/content/gdrive/MyDrive/ai_dataset/train.csv', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "id": "qyIwbYXzb3Ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf07714f-1d68-47ca-8830-449da6470e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.model_selection import train_test_split\n",
        "\n",
        "# 순차적 추출\n"
      ],
      "metadata": {
        "id": "24e7WsdFnOci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Model**"
      ],
      "metadata": {
        "id": "xxy_0z_Vfxr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepLabV3+ 구조 정의\n",
        "class DeepLabV3Plus(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DeepLabV3Plus, self).__init__()\n",
        "        self.backbone = models.resnet101(pretrained=True) # 할 수 있으면 backbone network Xception으로 수정\n",
        "        self.asspp = ASSPP() # Astrous Separable Spatial Pyramid convolution\n",
        "        self.decoder = Decoder() # Decoder\n",
        "        '''\n",
        "        구조의 마지막 layer인 로짓 생성 레이어의 정의부\n",
        "        ASSPP와 Decoder를 거친 feature map을 입력으로 받아 클래스 수에 해당하는 출력 채널 수를 가진 로짓 생성\n",
        "        '''\n",
        "        self.logits = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # backbone network - 입력 이미지에 대한 초기 전처리\n",
        "        x = self.backbone.conv1(x)\n",
        "        x = self.backbone.bn1(x)\n",
        "        x = self.backbone.relu(x)\n",
        "        x = self.backbone.maxpool(x)\n",
        "\n",
        "        # 연속된 layer를 통해 특징 추출\n",
        "        x = self.backbone.layer1(x)\n",
        "        x = self.backbone.layer2(x)\n",
        "        x = self.backbone.layer3(x)\n",
        "        x = self.backbone.layer4(x)\n",
        "\n",
        "        # segmentation\n",
        "        feat1, x = self.asspp(x)\n",
        "        x = self.decoder(feat1, x)\n",
        "        x = self.logits(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ASSPP(nn.Module): # Atrous Separable Spatial Pyramid Pooling\n",
        "    def __init__(self, in_channels=2048, out_channels=256):\n",
        "        super(ASSPP, self).__init__()\n",
        "        dilations = [1, 6, 12, 18] # rate\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1) # Atrous Conv\n",
        "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1,  padding=dilations[0] + 1, dilation=dilations[0]) # 1x1 Conv\n",
        "        self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=dilations[1] + 1, dilation=dilations[1]) # 3X3 Conv rate 6\n",
        "        self.conv4 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=dilations[2] + 1, dilation=dilations[2]) # 3x3 Conv rate 12\n",
        "        self.conv5 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=dilations[3] + 1, dilation=dilations[3]) # 3x3 Conv rate 18\n",
        "\n",
        "        # depthwise separable convolution\n",
        "        self.sep_conv1 = nn.Conv2d(out_channels * 5, out_channels, kernel_size=1) # 입력 채널 수 줄이기\n",
        "        self.sep_conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, groups=out_channels) # 각 채널 개별적으로 처리\n",
        "        self.sep_conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=1) # 출력 채널 수를 out_channels로 줄이기\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        convolution을 통해 처리된 feature map 결과들을 channel 차원을 기준으로 결합(torch.cat 이용)\n",
        "        '''\n",
        "        feat1 = self.conv1(x)\n",
        "        feat2 = self.conv2(x)\n",
        "        feat3 = self.conv3(x)\n",
        "        feat4 = self.conv4(x)\n",
        "        feat5 = self.conv5(x)\n",
        "\n",
        "        # 특성 맵들의 크기를 동일하게 조정한 후 concat\n",
        "        feat1 = nn.AdaptiveAvgPool2d((feat5.size(2), feat5.size(3)))(feat1)\n",
        "        feat2 = nn.AdaptiveAvgPool2d((feat5.size(2), feat5.size(3)))(feat2)\n",
        "        feat3 = nn.AdaptiveAvgPool2d((feat5.size(2), feat5.size(3)))(feat3)\n",
        "        feat4 = nn.AdaptiveAvgPool2d((feat5.size(2), feat5.size(3)))(feat4)\n",
        "\n",
        "        out = torch.cat([feat1, feat2, feat3, feat4, feat5], dim=1)\n",
        "\n",
        "        # 결합된 특성 맵은 sep_conv1을 통해 채널 방향으로 축소\n",
        "        # sep_conv2는 3x3커널을 사용, 그룹 내에서 채널별로 분리하여 공간 방향의 특성을 잡아냄\n",
        "        # sep_conv3를 통해 특성을 다시 축소한 후 ReLU 활성화 함수 적용해 최종 출력 얻어냄\n",
        "        out = self.sep_conv1(out)\n",
        "        out = self.sep_conv2(out)\n",
        "        out = self.sep_conv3(out)\n",
        "        out = self.relu(out)\n",
        "        return feat1, out\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels=256, out_channels=256, low_level_channels=256):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels + low_level_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, low_level_features):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = F.interpolate(out, scale_factor=4, mode='bilinear', align_corners=False)\n",
        "\n",
        "        # low_level_features 크기를 out과 일치시키기 위해 조정\n",
        "        low_level_features = nn.AdaptiveAvgPool2d((out.size(2), out.size(3)))(low_level_features)\n",
        "\n",
        "        out = torch.cat([out, low_level_features], dim=1)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "0ZH4pWqn6xuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Train**"
      ],
      "metadata": {
        "id": "OKt9zylqf0Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model 초기화\n",
        "model = DeepLabV3Plus(18).to(device)\n",
        "\n",
        "# loss function과 optimizer 정의\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(10):  # 10 에폭 동안 학습합니다.\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, masks in tqdm(dataloader):\n",
        "        images = images.float().to(device)\n",
        "        masks = masks.float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        '''\n",
        "        출력 크기는 모델의 클래스 수에 해당하는 차원\n",
        "        목표 크기는 이진 분류 작업을 수행하기 위해 채널 차원 1\n",
        "        '''\n",
        "\n",
        "        loss = criterion(outputs, masks.unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')"
      ],
      "metadata": {
        "id": "aqUhHq_bb9iR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "0451014c-81df-40ed-b899-a22d488b6158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6b0eb782204e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model 초기화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepLabV3Plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# loss function과 optimizer 정의\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.75 GiB total capacity; 13.29 GiB already allocated; 832.00 KiB free; 13.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference**"
      ],
      "metadata": {
        "id": "-pD2CtOSf49S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SatelliteDataset(csv_file='/content/gdrive/MyDrive/ai_dataset/test.csv', transform=transform, infer=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "hh9VB0EjcBQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    result = []\n",
        "    for images in tqdm(test_dataloader):\n",
        "        images = images.float().to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        masks = torch.sigmoid(outputs).cpu().numpy()\n",
        "        masks = np.squeeze(masks, axis=1)\n",
        "        masks = (masks > 0.35).astype(np.uint8) # Threshold = 0.35\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            mask_rle = rle_encode(masks[i])\n",
        "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
        "                result.append(-1)\n",
        "            else:\n",
        "                result.append(mask_rle)"
      ],
      "metadata": {
        "id": "CtPNk1E9cEhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Submisssion**"
      ],
      "metadata": {
        "id": "k5up68J8f7YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('/content/gdrive/MyDrive/ai_dataset/sample_submission.csv')\n",
        "submit['mask_rle'] = result"
      ],
      "metadata": {
        "id": "RJNQLImmcFB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit.to_csv('/content/gdrive/MyDrive/ai_dataset/submit.csv', index=False)"
      ],
      "metadata": {
        "id": "XVvSekaKcHNN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}